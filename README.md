# LSTM-Autoencoder
LSTM is combined with Autoencoders (AE) for time-series prediction 

LSTM is combined with Autoencoders (AE) for time-series prediction in another mode (LSTM-AE). This combination is inspired to increase the robustness of the model by capturing temporal dependencies in the data, which can be overlooked by other techniques for decreasing dimensionality, such as PCA. In this combination, the LSTM model can extract the temporal features from the input data and the AE, which is a type of neural network, can learn to represent input data in a compressed form, and then reconstruct the original input data from this compressed representation. AE reduces the dimensionality of these features, helps to remove noise, and improve the robustness of the model. In this model, the input and the output are  reshaped into the required 3-dimensional format (samples, timesteps, features), and the data is converted into PyTorch tensors.  The model specifies the AutoEncoder class, which contains an encoder and decoder. Both the encoder and the decoder are implemented as LSTM networks. Encoder decreases the dimensionality of input data and decoder reconstructs original information from that reduced representation. The LSTM_AE class is a combination of the Autoencoder and other LSTM layers. It uses an AE to encode and decode the input information, then sends those encoded outputs to a LSTM layer. A complete connected linear layer is used to produce the final output. The grid search parameters are specified after that. The code will update, instantiate and train a model for each combination of these parameters. It is calculating RMSE for model predictions on the test data, keeping track of the parameters that fall under the lowest RMSE.
